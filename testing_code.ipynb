{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8eaac0-9c4c-4f7a-a8cd-2e1824a4ffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elroy\\gesture_env\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∏ Starting real-time gesture recognition...\n",
      "Available gestures: ['swipe_left', 'swipe_right', 'swipe_up', 'swipe_down', 'screenshot', 'drop', 'like', 'dislike', 'sos']\n",
      "‚ùå Press 'q' to quit.\n",
      "Detected: dislike (Confidence: 0.92, Cooldown: 2.0s)\n",
      "Detected: swipe_up (Confidence: 0.99, Cooldown: 2.0s)\n",
      "Detected: swipe_right (Confidence: 1.00, Cooldown: 2.0s)\n",
      "Detected: swipe_left (Confidence: 1.00, Cooldown: 2.0s)\n",
      "Detected: swipe_right (Confidence: 0.84, Cooldown: 2.0s)\n",
      "Detected: screenshot (Confidence: 0.98, Cooldown: 2.0s)\n",
      "Detected: drop (Confidence: 1.00, Cooldown: 2.0s)\n",
      "Detected: screenshot (Confidence: 1.00, Cooldown: 2.0s)\n",
      "Detected: drop (Confidence: 0.99, Cooldown: 2.0s)\n",
      "Detected: screenshot (Confidence: 1.00, Cooldown: 2.0s)\n",
      "Detected: swipe_down (Confidence: 0.97, Cooldown: 2.0s)\n",
      "Detected: drop (Confidence: 1.00, Cooldown: 2.0s)\n",
      "Detected: screenshot (Confidence: 0.96, Cooldown: 2.0s)\n",
      "Detected: drop (Confidence: 1.00, Cooldown: 2.0s)\n",
      "üü¢ Testing stopped.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "# ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "SEQUENCE_LENGTH = 60\n",
    "LANDMARKS = 63\n",
    "GESTURE_LABELS = np.load(\"models/labels.npy\").tolist()\n",
    "NUM_CLASSES = len(GESTURE_LABELS)\n",
    "STATIC_GESTURES = [\"like\", \"dislike\", \"peace\"]\n",
    "MIN_CONFIDENCE = 0.80  # For high precision\n",
    "COOLDOWN_SECONDS = 2.0  # For faster gesture cycling\n",
    "PREDICTION_WINDOW = 7  # For stable predictions\n",
    "MAJORITY_THRESHOLD = 0.7  # At least 70% agreement\n",
    "\n",
    "# ‚îÄ‚îÄ INIT MEDIAPIPE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.75,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# ‚îÄ‚îÄ LOAD TFLITE MODEL ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "interpreter = tf.lite.Interpreter(model_path=\"models/gesture_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ INIT CAMERA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "cap.set(cv2.CAP_PROP_FPS, 60)\n",
    "buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "prediction_history = deque(maxlen=PREDICTION_WINDOW)\n",
    "fps_counter = deque(maxlen=30)\n",
    "\n",
    "# Cooldown and gesture state\n",
    "cooldown_active = False\n",
    "cooldown_end_time = 0\n",
    "current_gesture = None\n",
    "\n",
    "def start_cooldown(gesture_name):\n",
    "    \"\"\"Start cooldown period for a detected gesture.\"\"\"\n",
    "    global cooldown_active, cooldown_end_time, current_gesture\n",
    "    cooldown_active = True\n",
    "    cooldown_end_time = time.time() + COOLDOWN_SECONDS\n",
    "    current_gesture = gesture_name\n",
    "    return COOLDOWN_SECONDS\n",
    "\n",
    "def check_cooldown():\n",
    "    \"\"\"Check current cooldown status.\"\"\"\n",
    "    global cooldown_active\n",
    "    if not cooldown_active:\n",
    "        return False\n",
    "    time_remaining = cooldown_end_time - time.time()\n",
    "    if time_remaining <= 0:\n",
    "        cooldown_active = False\n",
    "        return False\n",
    "    return time_remaining\n",
    "\n",
    "def preprocess_sequence(sequence, gesture_name):\n",
    "    \"\"\"Preprocess sequence to match training data (only normalized landmarks).\"\"\"\n",
    "    sequence = np.array(sequence, dtype=np.float32)\n",
    "    \n",
    "    # Normalize by subtracting the first frame\n",
    "    sequence_norm = sequence - sequence[0]\n",
    "    \n",
    "    # For static gestures, compute mean pose over middle frames\n",
    "    if gesture_name in STATIC_GESTURES:\n",
    "        middle_frames = sequence_norm[20:40]\n",
    "        mean_pose = np.mean(middle_frames, axis=0, keepdims=True)\n",
    "        sequence_norm = np.repeat(mean_pose, SEQUENCE_LENGTH, axis=0)\n",
    "    \n",
    "    return sequence_norm  # Shape: (SEQUENCE_LENGTH, 63)\n",
    "\n",
    "print(\"üì∏ Starting real-time gesture recognition...\")\n",
    "print(f\"Available gestures: {GESTURE_LABELS}\")\n",
    "print(\"‚ùå Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    start_frame_time = time.time()\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"‚ùå Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    landmark_row = None\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmark_row = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmark_row.extend([lm.x, lm.y, lm.z])\n",
    "            break\n",
    "\n",
    "    # ‚îÄ‚îÄ COLLECT SEQUENCE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if landmark_row:\n",
    "        buffer.append(landmark_row)\n",
    "    else:\n",
    "        buffer.append([0] * LANDMARKS)\n",
    "        cv2.putText(frame, \"No hand detected\", (10, 80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        # Reset buffer if no hand detected for 15+ frames\n",
    "        if len(buffer) == SEQUENCE_LENGTH and all(np.all(b == 0) for b in list(buffer)[-15:]):\n",
    "            buffer.clear()\n",
    "            prediction_history.clear()\n",
    "\n",
    "    # Check cooldown\n",
    "    cooldown_remaining = check_cooldown()\n",
    "    if cooldown_active and cooldown_remaining:\n",
    "        cv2.putText(frame, f\"{current_gesture} ({cooldown_remaining:.1f}s)\", (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 128, 255), 2)\n",
    "    \n",
    "    # ‚îÄ‚îÄ INFERENCE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    elif len(buffer) == SEQUENCE_LENGTH:\n",
    "        sequence = np.array(buffer)\n",
    "        if sequence.shape == (SEQUENCE_LENGTH, LANDMARKS) and not np.all(sequence == 0):\n",
    "            try:\n",
    "                # Preprocess sequence\n",
    "                sequence_norm = preprocess_sequence(sequence, \"\")\n",
    "                \n",
    "                # Prepare input for TFLite model\n",
    "                input_data = sequence_norm[np.newaxis, ...].astype(np.float32)\n",
    "                interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "                \n",
    "                # Run inference\n",
    "                interpreter.invoke()\n",
    "                output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "                \n",
    "                # Get prediction\n",
    "                pred_idx = np.argmax(output_data[0])\n",
    "                confidence = output_data[0][pred_idx]\n",
    "                \n",
    "                # Add to prediction history\n",
    "                prediction_history.append((pred_idx, confidence))\n",
    "                \n",
    "                # Temporal smoothing and majority voting\n",
    "                if len(prediction_history) >= PREDICTION_WINDOW:\n",
    "                    pred_indices = [idx for idx, _ in prediction_history]\n",
    "                    counter = {}\n",
    "                    for idx in pred_indices:\n",
    "                        counter[idx] = counter.get(idx, 0) + 1\n",
    "                    \n",
    "                    total_votes = sum(counter.values())\n",
    "                    most_common = max(counter.items(), key=lambda x: x[1])\n",
    "                    final_pred_idx = most_common[0]\n",
    "                    vote_proportion = most_common[1] / total_votes\n",
    "                    \n",
    "                    if vote_proportion >= MAJORITY_THRESHOLD:\n",
    "                        confidences = [conf for idx, conf in prediction_history if idx == final_pred_idx]\n",
    "                        avg_confidence = sum(confidences) / len(confidences)\n",
    "                        \n",
    "                        if avg_confidence > MIN_CONFIDENCE:\n",
    "                            pred_label = GESTURE_LABELS[final_pred_idx]\n",
    "                            \n",
    "                            # Display prediction\n",
    "                            text = f\"Gesture: {pred_label} ({avg_confidence:.2f})\"\n",
    "                            cv2.putText(frame, text, (10, 40),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                            \n",
    "                            # Start cooldown\n",
    "                            cooldown_duration = start_cooldown(pred_label)\n",
    "                            print(f\"Detected: {pred_label} (Confidence: {avg_confidence:.2f}, Cooldown: {cooldown_duration}s)\")\n",
    "                            \n",
    "                            # Clear history after detection\n",
    "                            prediction_history.clear()\n",
    "                        else:\n",
    "                            cv2.putText(frame, f\"Low confidence ({avg_confidence:.2f})\", (10, 40),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 165, 0), 2)\n",
    "                    else:\n",
    "                        cv2.putText(frame, \"Uncertain prediction\", (10, 40),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (200, 200, 200), 2)\n",
    "                else:\n",
    "                    cv2.putText(frame, \"Analyzing...\", (10, 40),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (200, 200, 200), 2)\n",
    "            except Exception as e:\n",
    "                cv2.putText(frame, f\"Error: {str(e)[:20]}\", (10, 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                print(f\"Error during inference: {str(e)}\")\n",
    "        else:\n",
    "            cv2.putText(frame, \"Invalid sequence\", (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Calculate and display FPS\n",
    "    frame_time = time.time() - start_frame_time\n",
    "    fps = 1.0 / max(frame_time, 0.001)\n",
    "    fps_counter.append(fps)\n",
    "    avg_fps = sum(fps_counter) / len(fps_counter)\n",
    "    \n",
    "    cv2.putText(frame, f\"FPS: {avg_fps:.1f}\", (frame.shape[1] - 120, 25),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Gesture Recognition\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# ‚îÄ‚îÄ CLEANUP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n",
    "print(\"üü¢ Testing stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db80e22-060d-4097-ab96-d1af4fc8c15a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "üñê Gesture Env",
   "language": "python",
   "name": "gesture_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
