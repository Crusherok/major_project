{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d9ec2-7727-48c8-84b0-a5f70430b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "SEQUENCE_LENGTH = 60\n",
    "LANDMARKS = 63  # 21 landmarks × 3 coordinates (x, y, z)\n",
    "MODEL_PATH = \"gesture_model.tflite\"\n",
    "LABELS_PATH = \"labels.npy\"\n",
    "\n",
    "# Recognition parameters (can be adjusted based on preferences)\n",
    "CONFIDENCE_THRESHOLD = 0.70  # Default confidence threshold\n",
    "DETECTION_DURATION = 2.0  # Time a gesture must be held before confirming (seconds)\n",
    "IDLE_COOLDOWN = 1.5  # Time before detecting another gesture (seconds)\n",
    "INITIAL_DELAY = 1.0  # Startup delay before detecting gestures (seconds)\n",
    "STATIC_GESTURE_THRESHOLD = 0.85  # Higher threshold for static gestures\n",
    "DYNAMIC_GESTURE_THRESHOLD = 0.65  # Lower threshold for dynamic gestures\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Check if model exists, show appropriate message if not\n",
    "if not os.path.exists(MODEL_PATH) or not os.path.exists(LABELS_PATH):\n",
    "    print(\"⚠️ Model files not found! Please ensure gesture_model.tflite and labels.npy are in the models directory.\")\n",
    "    print(\"If you're running this for the first time, please run the training script first.\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the TFLite model\n",
    "print(\"Loading model...\")\n",
    "interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Load gesture labels\n",
    "labels = np.load(LABELS_PATH, allow_pickle=True).tolist()\n",
    "print(f\"Loaded {len(labels)} gesture labels: {labels}\")\n",
    "\n",
    "# Identify static and dynamic gestures (assuming the same classification as in training)\n",
    "STATIC_GESTURES = [\"like\", \"dislike\", \"peace\"]\n",
    "DYNAMIC_GESTURES = [g for g in labels if g not in STATIC_GESTURES]\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Set up video capture\n",
    "print(\"Initializing camera...\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)  # Try to get 30fps if possible\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Error: Could not open video capture device\")\n",
    "    exit()\n",
    "\n",
    "# Sequence buffers\n",
    "raw_sequence = deque(maxlen=SEQUENCE_LENGTH)\n",
    "norm_sequence = deque(maxlen=SEQUENCE_LENGTH)\n",
    "velocity_sequence = deque(maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "# Feature extraction functions (matching the training preprocessing)\n",
    "def extract_features(landmarks_sequence):\n",
    "    \"\"\"Extract normalized position and velocity features from landmarks\"\"\"\n",
    "    if len(landmarks_sequence) < SEQUENCE_LENGTH:\n",
    "        return None, None\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    seq_array = np.array(landmarks_sequence, dtype=np.float32)\n",
    "    \n",
    "    # Normalize relative to first frame\n",
    "    seq_normalized = seq_array - seq_array[0]\n",
    "    \n",
    "    # Calculate velocity (temporal derivatives)\n",
    "    seq_velocity = np.zeros_like(seq_normalized)\n",
    "    seq_velocity[1:] = seq_normalized[1:] - seq_normalized[:-1]\n",
    "    \n",
    "    return seq_normalized, seq_velocity\n",
    "\n",
    "# State tracking\n",
    "current_gesture = None\n",
    "gesture_start_time = None\n",
    "last_detection_time = 0\n",
    "program_start_time = time.time()\n",
    "no_hand_detected_time = None\n",
    "detection_history = []\n",
    "total_frames = 0\n",
    "fps_history = deque(maxlen=30)\n",
    "prediction_history = deque(maxlen=10)  # Store recent predictions for smoothing\n",
    "start_time = time.time()\n",
    "\n",
    "# Visualization colors\n",
    "COLORS = {\n",
    "    'background': (50, 50, 50),\n",
    "    'text': (255, 255, 255),\n",
    "    'detecting': (50, 205, 50),   # Green\n",
    "    'confirmed': (0, 191, 255),   # Deep Sky Blue\n",
    "    'waiting': (255, 165, 0),     # Orange\n",
    "    'no_hand': (128, 128, 128),   # Gray\n",
    "    'high_conf': (50, 205, 50),   # Green\n",
    "    'medium_conf': (255, 165, 0), # Orange\n",
    "    'low_conf': (255, 0, 0)       # Red\n",
    "}\n",
    "\n",
    "# Smoothing function for predictions\n",
    "def smooth_predictions(new_pred, history=None):\n",
    "    if history is None:\n",
    "        return new_pred\n",
    "    \n",
    "    # Add current prediction to history\n",
    "    history.append(new_pred)\n",
    "    \n",
    "    # Calculate weighted average (more recent predictions have higher weight)\n",
    "    weights = np.linspace(0.5, 1.0, len(history))\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    # Combine predictions\n",
    "    combined = {}\n",
    "    for i, pred in enumerate(history):\n",
    "        for item in pred:\n",
    "            label = item[\"label\"]\n",
    "            conf = item[\"confidence\"] * weights[i]\n",
    "            if label in combined:\n",
    "                combined[label] += conf\n",
    "            else:\n",
    "                combined[label] = conf\n",
    "    \n",
    "    # Sort and format result\n",
    "    result = [{\"label\": k, \"confidence\": v} for k, v in combined.items()]\n",
    "    result.sort(key=lambda x: x[\"confidence\"], reverse=True)\n",
    "    \n",
    "    return result[:3]  # Return top 3\n",
    "\n",
    "# Function to create visualization of confidence scores\n",
    "def draw_confidence_bars(frame, predictions, x=20, y=80, width=200, height=20, gap=30):\n",
    "    for i, pred in enumerate(predictions[:3]):  # Show top 3 predictions\n",
    "        confidence = pred[\"confidence\"]\n",
    "        label = pred[\"label\"]\n",
    "        \n",
    "        # Determine color based on confidence\n",
    "        if confidence > 0.8:\n",
    "            color = COLORS['high_conf']\n",
    "        elif confidence > 0.5:\n",
    "            color = COLORS['medium_conf']\n",
    "        else:\n",
    "            color = COLORS['low_conf']\n",
    "        \n",
    "        # Draw bar background\n",
    "        cv2.rectangle(frame, (x, y + i*gap), (x + width, y + height + i*gap), (50, 50, 50), -1)\n",
    "        \n",
    "        # Draw filled portion of bar\n",
    "        filled_width = int(width * confidence)\n",
    "        cv2.rectangle(frame, (x, y + i*gap), (x + filled_width, y + height + i*gap), color, -1)\n",
    "        \n",
    "        # Draw border\n",
    "        cv2.rectangle(frame, (x, y + i*gap), (x + width, y + height + i*gap), (200, 200, 200), 1)\n",
    "        \n",
    "        # Add label and percentage\n",
    "        text = f\"{label}: {confidence*100:.1f}%\"\n",
    "        cv2.putText(frame, text, (x + width + 10, y + height//2 + i*gap + 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS['text'], 1)\n",
    "\n",
    "# Create gesture overlay with more information\n",
    "def create_gesture_overlay(frame, text, predictions=None, fps=0, progress=None, status=\"idle\"):\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    # Create header bar\n",
    "    header_height = 60\n",
    "    if status == \"detecting\":\n",
    "        color = COLORS['detecting']\n",
    "    elif status == \"confirmed\":\n",
    "        color = COLORS['confirmed']\n",
    "    elif status == \"waiting\":\n",
    "        color = COLORS['waiting']\n",
    "    elif status == \"no_hand\":\n",
    "        color = COLORS['no_hand']\n",
    "    else:\n",
    "        color = COLORS['background']\n",
    "    \n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (0, 0), (w, header_height), color, -1)\n",
    "    \n",
    "    # Apply opacity\n",
    "    alpha = 0.7\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "    \n",
    "    # Add main text\n",
    "    cv2.putText(frame, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, COLORS['text'], 2)\n",
    "    \n",
    "    # Add FPS counter\n",
    "    fps_text = f\"FPS: {fps:.1f}\"\n",
    "    cv2.putText(frame, fps_text, (w - 120, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.6, COLORS['text'], 1)\n",
    "    \n",
    "    # Add progress bar if provided\n",
    "    if progress is not None:\n",
    "        bar_width = 200\n",
    "        bar_height = 5\n",
    "        bar_x = w - bar_width - 20\n",
    "        bar_y = 20\n",
    "        \n",
    "        # Background\n",
    "        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (100, 100, 100), -1)\n",
    "        \n",
    "        # Filled portion\n",
    "        filled_width = int(bar_width * progress)\n",
    "        cv2.rectangle(frame, (bar_x, bar_y), (bar_x + filled_width, bar_y + bar_height), (255, 255, 255), -1)\n",
    "    \n",
    "    # Draw confidence bars if predictions are provided\n",
    "    if predictions:\n",
    "        draw_confidence_bars(frame, predictions)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "# Logging function\n",
    "def log_detection(gesture, confidence):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"{timestamp},{gesture},{confidence:.4f}\\n\"\n",
    "    with open(f\"logs/gesture_log_{datetime.now().strftime('%Y%m%d')}.csv\", \"a\") as f:\n",
    "        f.write(log_entry)\n",
    "\n",
    "# Main loop\n",
    "print(\"Starting gesture recognition...\\n\")\n",
    "print(\"Press 'q' to quit\")\n",
    "print(\"Press 's' to take a screenshot\")\n",
    "print(\"Press 'd' to toggle debug visualization\")\n",
    "\n",
    "show_debug = False\n",
    "debug_plot = None\n",
    "debug_frames = []\n",
    "debug_confidences = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame from camera. Exiting...\")\n",
    "        break\n",
    "\n",
    "    # Calculate FPS\n",
    "    current_time = time.time()\n",
    "    elapsed = current_time - start_time\n",
    "    start_time = current_time\n",
    "    fps = 1 / elapsed if elapsed > 0 else 0\n",
    "    fps_history.append(fps)\n",
    "    avg_fps = sum(fps_history) / len(fps_history)\n",
    "    \n",
    "    total_frames += 1\n",
    "    frame = cv2.flip(frame, 1)  # Mirror image for more intuitive feedback\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply initial delay before detection starts\n",
    "    if current_time < program_start_time + INITIAL_DELAY:\n",
    "        progress = (current_time - program_start_time) / INITIAL_DELAY\n",
    "        frame = create_gesture_overlay(frame, \"INITIALIZING SYSTEM...\", \n",
    "                                      fps=avg_fps, progress=progress, status=\"waiting\")\n",
    "        cv2.imshow(\"Gesture Recognition\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        continue\n",
    "\n",
    "    # Process hand landmarks\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # Handle case when no hand is detected\n",
    "    if not results.multi_hand_landmarks:\n",
    "        if no_hand_detected_time is None:\n",
    "            no_hand_detected_time = current_time\n",
    "            # Clear the sequence buffers when hand disappears\n",
    "            raw_sequence.clear()\n",
    "            norm_sequence.clear()\n",
    "            velocity_sequence.clear()\n",
    "\n",
    "        # Only show \"NO HAND DETECTED\" after a short delay\n",
    "        if current_time - no_hand_detected_time > 1.0:\n",
    "            frame = create_gesture_overlay(frame, \"NO HAND DETECTED\", \n",
    "                                          fps=avg_fps, status=\"no_hand\")\n",
    "        else:\n",
    "            frame = create_gesture_overlay(frame, \"WAITING FOR HAND...\", \n",
    "                                          fps=avg_fps, status=\"waiting\")\n",
    "        \n",
    "        cv2.imshow(\"Gesture Recognition\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            screenshot_path = f\"screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "            cv2.imwrite(screenshot_path, frame)\n",
    "            print(f\"Screenshot saved to {screenshot_path}\")\n",
    "        elif key == ord('d'):\n",
    "            show_debug = not show_debug\n",
    "            if not show_debug and debug_plot is not None:\n",
    "                plt.close(debug_plot)\n",
    "                debug_plot = None\n",
    "        \n",
    "        continue\n",
    "\n",
    "    # Hand is detected\n",
    "    no_hand_detected_time = None\n",
    "    \n",
    "    # Process first detected hand\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_draw.draw_landmarks(\n",
    "            frame, \n",
    "            hand_landmarks, \n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_draw.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "            connection_drawing_spec=mp_draw.DrawingSpec(color=(0, 255, 0), thickness=1)\n",
    "        )\n",
    "        \n",
    "        # Extract landmarks\n",
    "        landmarks = []\n",
    "        for lm in hand_landmarks.landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "        \n",
    "        # Store raw landmarks\n",
    "        raw_sequence.append(landmarks)\n",
    "        break  # Process only the first hand\n",
    "\n",
    "    # Feature extraction and prediction\n",
    "    if len(raw_sequence) == SEQUENCE_LENGTH:\n",
    "        # Extract features matching the training preprocessing\n",
    "        seq_normalized, seq_velocity = extract_features(raw_sequence)\n",
    "        \n",
    "        if seq_normalized is not None:\n",
    "            # Prepare input tensor\n",
    "            input_data = seq_normalized[np.newaxis, ...]\n",
    "            \n",
    "            # Run inference\n",
    "            interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "            interpreter.invoke()\n",
    "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "            \n",
    "            # Get top predictions\n",
    "            top_indices = output_data[0].argsort()[-3:][::-1]\n",
    "            top_predictions = [{\"label\": labels[idx], \"confidence\": float(output_data[0][idx])} \n",
    "                              for idx in top_indices]\n",
    "            \n",
    "            # Apply different thresholds based on gesture type\n",
    "            top_gesture = top_predictions[0][\"label\"]\n",
    "            top_confidence = top_predictions[0][\"confidence\"]\n",
    "            \n",
    "            # Determine threshold based on gesture type\n",
    "            if top_gesture in STATIC_GESTURES:\n",
    "                current_threshold = STATIC_GESTURE_THRESHOLD\n",
    "            else:\n",
    "                current_threshold = DYNAMIC_GESTURE_THRESHOLD\n",
    "            \n",
    "            # Apply smoothing\n",
    "            smoothed_predictions = smooth_predictions(top_predictions, prediction_history)\n",
    "            top_smoothed = smoothed_predictions[0]\n",
    "            \n",
    "            # For debugging, record confidence values\n",
    "            if show_debug:\n",
    "                debug_frames.append(total_frames)\n",
    "                debug_confidences.append(top_smoothed[\"confidence\"])\n",
    "                \n",
    "                # Keep only the last 100 frames\n",
    "                if len(debug_frames) > 100:\n",
    "                    debug_frames.pop(0)\n",
    "                    debug_confidences.pop(0)\n",
    "            \n",
    "            # Gesture recognition logic\n",
    "            if top_smoothed[\"confidence\"] > current_threshold:\n",
    "                if current_gesture is None or current_gesture != top_smoothed[\"label\"]:\n",
    "                    current_gesture = top_smoothed[\"label\"]\n",
    "                    gesture_start_time = current_time\n",
    "                \n",
    "                # Calculate detection duration\n",
    "                detection_duration = current_time - gesture_start_time\n",
    "                \n",
    "                # Handle confirmation process\n",
    "                if detection_duration < DETECTION_DURATION:\n",
    "                    # Still in detection phase\n",
    "                    progress = detection_duration / DETECTION_DURATION\n",
    "                    overlay_text = f\"DETECTING: {current_gesture}\"\n",
    "                    frame = create_gesture_overlay(frame, overlay_text, \n",
    "                                                 predictions=smoothed_predictions,\n",
    "                                                 fps=avg_fps, progress=progress,\n",
    "                                                 status=\"detecting\")\n",
    "                elif current_time - last_detection_time > IDLE_COOLDOWN:\n",
    "                    # Confirm the gesture\n",
    "                    last_detection_time = current_time\n",
    "                    overlay_text = f\"CONFIRMED: {current_gesture.upper()}\"\n",
    "                    frame = create_gesture_overlay(frame, overlay_text, \n",
    "                                                 predictions=smoothed_predictions,\n",
    "                                                 fps=avg_fps,\n",
    "                                                 status=\"confirmed\")\n",
    "                    \n",
    "                    # Log the detection\n",
    "                    log_detection(current_gesture, top_smoothed[\"confidence\"])\n",
    "                    detection_history.append((current_time, current_gesture))\n",
    "                    \n",
    "                    # Reset current gesture after confirmation\n",
    "                    current_gesture = None\n",
    "                else:\n",
    "                    # In cooldown period\n",
    "                    cooldown_remaining = IDLE_COOLDOWN - (current_time - last_detection_time)\n",
    "                    progress = 1.0 - (cooldown_remaining / IDLE_COOLDOWN)\n",
    "                    overlay_text = f\"COOLDOWN: {cooldown_remaining:.1f}s\"\n",
    "                    frame = create_gesture_overlay(frame, overlay_text, \n",
    "                                                 predictions=smoothed_predictions,\n",
    "                                                 fps=avg_fps, progress=progress,\n",
    "                                                 status=\"waiting\")\n",
    "            else:\n",
    "                frame = create_gesture_overlay(frame, \"READY\", \n",
    "                                              predictions=smoothed_predictions,\n",
    "                                              fps=avg_fps)\n",
    "                current_gesture = None\n",
    "    else:\n",
    "        # Not enough frames yet\n",
    "        progress = len(raw_sequence) / SEQUENCE_LENGTH\n",
    "        frame = create_gesture_overlay(frame, f\"BUFFERING FRAMES: {len(raw_sequence)}/{SEQUENCE_LENGTH}\", \n",
    "                                     fps=avg_fps, progress=progress, status=\"waiting\")\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Gesture Recognition\", frame)\n",
    "    \n",
    "    # Show debug visualization if enabled\n",
    "    if show_debug and len(debug_frames) > 10:\n",
    "        if debug_plot is None or not plt.fignum_exists(debug_plot.number):\n",
    "            debug_plot = plt.figure(figsize=(10, 4))\n",
    "            plt.ion()\n",
    "        \n",
    "        plt.clf()\n",
    "        plt.plot(debug_frames, debug_confidences, 'b-')\n",
    "        plt.axhline(y=STATIC_GESTURE_THRESHOLD, color='r', linestyle='-', label='Static threshold')\n",
    "        plt.axhline(y=DYNAMIC_GESTURE_THRESHOLD, color='g', linestyle='-', label='Dynamic threshold')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.title(\"Gesture Confidence Over Time\")\n",
    "        plt.xlabel(\"Frame\")\n",
    "        plt.ylabel(\"Confidence\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.draw()\n",
    "        plt.pause(0.001)\n",
    "    \n",
    "    # Check for key presses\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('s'):\n",
    "        screenshot_path = f\"screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "        cv2.imwrite(screenshot_path, frame)\n",
    "        print(f\"Screenshot saved to {screenshot_path}\")\n",
    "    elif key == ord('d'):\n",
    "        show_debug = not show_debug\n",
    "        if not show_debug and debug_plot is not None:\n",
    "            plt.close(debug_plot)\n",
    "            debug_plot = None\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "if debug_plot is not None:\n",
    "    plt.close(debug_plot)\n",
    "\n",
    "print(\"Gesture recognition ended.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "🖐 Gesture Env",
   "language": "python",
   "name": "gesture_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
